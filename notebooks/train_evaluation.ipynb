{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exploring how emojis affect sentiment analysis - Algorithms.ipynb","provenance":[],"collapsed_sections":["-VWyAoDyUFDY"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HRn1BHd1_0jT"},"source":["# Exploring how emojis affect sentiment analysis\n","## Introduction\n","The aim of this research is to understand how emoticons and emojis can influence the polarity or sentimentof a sentence in the domain of Sentiment Analysis.\n","\n","In order to achieve this aim, the research objectives are:\n","- Conduct a research of how traditional machine learning techniques are applied to the task of Sentiment Analysis. \n","- Conduct a research of public datasets containing tweets or reviews which their sentiment manually assigned.\n","- Implement machine learning techniques and make experiments between them within the selected dataset.\n","- Explain how the presence of emojis and emoticons influences the results. In case of conflict of polarity between the sentence and the emoji, aim to understand if the text is ironic.\n","\n","## Useful Links\n","- [Notion](https://www.notion.so/Research-Methodology-Project-Planning-d4631470aa3a41a5a31614b38937ccc9): Documents of the research including papers, datasets and implementations.\n","- [Selected dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment): The Twitter US Airline Sentiment dataset stored how travelers in February 2015 expressed their feelings of each major U.S.airline on Twitter. The text was manually labeled by contributors into positive, negative and neutral tweets.\n","\n","## Authors\n","- Serghei Socolovschi [serghei@kth.se](mailto:serghei@kth.se)\n","- Angel Igareta [alih2@kth.se](mailto:alih2@kth.se)"]},{"cell_type":"markdown","metadata":{"id":"SGFMk5KaB3-G"},"source":["## General"]},{"cell_type":"markdown","metadata":{"id":"rbs9HxJ5B5O0"},"source":["### Imports\n","All the imports of the folder should be here so it is more scalable and there are not redundancies nor different versions."]},{"cell_type":"code","metadata":{"id":"WTG2Ygmu8ln1"},"source":["# Load the TensorBoard notebook extension\n","%reload_ext tensorboard\n","!rm -rf ./logs/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbmr037kzw3o","executionInfo":{"status":"ok","timestamp":1610135545464,"user_tz":0,"elapsed":61936,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"60e792bf-00b1-4967-85f8-8fe1fc8f82d6"},"source":["# Run only once\n","!sudo apt install openjdk-8-jdk\n","!sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n","!pip install language-check\n","!pip install pycontractions\n","!pip install emot\n","!pip install demoji\n","!pip install pyspellchecker"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n","  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n","  libgtk2.0-common libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre\n","  openjdk-8-jre-headless x11-utils\n","Suggested packages:\n","  gvfs openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin libnss-mdns\n","  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n","  fonts-wqy-zenhei fonts-indic mesa-utils\n","The following NEW packages will be installed:\n","  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n","  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n","  libgtk2.0-common libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless\n","  openjdk-8-jre openjdk-8-jre-headless x11-utils\n","0 upgraded, 15 newly installed, 0 to remove and 16 not upgraded.\n","Need to get 43.4 MB of archives.\n","After this operation, 163 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre-headless amd64 8u275-b01-0ubuntu1~18.04 [28.2 MB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u275-b01-0ubuntu1~18.04 [69.7 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk-headless amd64 8u275-b01-0ubuntu1~18.04 [8,269 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u275-b01-0ubuntu1~18.04 [1,600 kB]\n","Fetched 43.4 MB in 3s (15.6 MB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 15.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libxxf86dga1:amd64.\n","(Reading database ... 145483 files and directories currently installed.)\n","Preparing to unpack .../00-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n","Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n","Selecting previously unselected package fonts-dejavu-core.\n","Preparing to unpack .../01-fonts-dejavu-core_2.37-1_all.deb ...\n","Unpacking fonts-dejavu-core (2.37-1) ...\n","Selecting previously unselected package fonts-dejavu-extra.\n","Preparing to unpack .../02-fonts-dejavu-extra_2.37-1_all.deb ...\n","Unpacking fonts-dejavu-extra (2.37-1) ...\n","Selecting previously unselected package x11-utils.\n","Preparing to unpack .../03-x11-utils_7.7+3build1_amd64.deb ...\n","Unpacking x11-utils (7.7+3build1) ...\n","Selecting previously unselected package libatk-wrapper-java.\n","Preparing to unpack .../04-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\n","Unpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n","Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n","Preparing to unpack .../05-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\n","Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n","Selecting previously unselected package libgtk2.0-common.\n","Preparing to unpack .../06-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n","Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n","Selecting previously unselected package libgtk2.0-0:amd64.\n","Preparing to unpack .../07-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n","Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n","Selecting previously unselected package libgail18:amd64.\n","Preparing to unpack .../08-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n","Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n","Selecting previously unselected package libgail-common:amd64.\n","Preparing to unpack .../09-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n","Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n","Selecting previously unselected package libgtk2.0-bin.\n","Preparing to unpack .../10-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n","Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","Preparing to unpack .../11-openjdk-8-jre-headless_8u275-b01-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n","Selecting previously unselected package openjdk-8-jre:amd64.\n","Preparing to unpack .../12-openjdk-8-jre_8u275-b01-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jre:amd64 (8u275-b01-0ubuntu1~18.04) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../13-openjdk-8-jdk-headless_8u275-b01-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n","Selecting previously unselected package openjdk-8-jdk:amd64.\n","Preparing to unpack .../14-openjdk-8-jdk_8u275-b01-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-8-jdk:amd64 (8u275-b01-0ubuntu1~18.04) ...\n","Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n","Setting up fonts-dejavu-core (2.37-1) ...\n","Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n","Setting up fonts-dejavu-extra (2.37-1) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n","Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n","Setting up openjdk-8-jdk-headless:amd64 (8u275-b01-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","Setting up x11-utils (7.7+3build1) ...\n","Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n","Setting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n","Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n","Setting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n","Setting up openjdk-8-jre:amd64 (8u275-b01-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n","Setting up openjdk-8-jdk:amd64 (8u275-b01-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n","Collecting language-check\n","  Downloading https://files.pythonhosted.org/packages/97/45/0fd1d3683d6129f30fa09143fa383cdf6dff8bc0d1648f2cf156109cb772/language-check-1.1.tar.gz\n","Building wheels for collected packages: language-check\n","  Building wheel for language-check (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for language-check: filename=language_check-1.1-cp36-none-any.whl size=90190900 sha256=e14c7990c0cd10ba4deb1a32cadb01e0917242577a14c353d8a342190bb4ffbd\n","  Stored in directory: /root/.cache/pip/wheels/d5/46/82/90a89c23eac1837364ed7217a9eed71bc9e6ad4825be93968e\n","Successfully built language-check\n","Installing collected packages: language-check\n","Successfully installed language-check-1.1\n","Collecting pycontractions\n","  Downloading https://files.pythonhosted.org/packages/a6/f5/d3ec9491c530cbc03af32ca2c6b69b0e89660daeb2856b485d90f9d82e5e/pycontractions-2.0.1-py3-none-any.whl\n","Requirement already satisfied: pyemd>=0.4.4 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (0.5.1)\n","Requirement already satisfied: language-check>=1.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (1.1)\n","Requirement already satisfied: gensim>=2.0 in /usr/local/lib/python3.6/dist-packages (from pycontractions) (3.6.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pyemd>=0.4.4->pycontractions) (1.19.4)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (4.0.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=2.0->pycontractions) (1.4.1)\n","Installing collected packages: pycontractions\n","Successfully installed pycontractions-2.0.1\n","Collecting emot\n","  Downloading https://files.pythonhosted.org/packages/49/07/20001ade19873de611b7b66a4d5e5aabbf190d65abea337d5deeaa2bc3de/emot-2.1-py3-none-any.whl\n","Installing collected packages: emot\n","Successfully installed emot-2.1\n","Collecting demoji\n","  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n","Installing collected packages: colorama, demoji\n","Successfully installed colorama-0.4.4 demoji-0.4.0\n","Collecting pyspellchecker\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/9d/5bb403decde661abc6c5467319a0729d7c238e04d8217d9fef885510ec9d/pyspellchecker-0.5.6-py2.py3-none-any.whl (2.5MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.5MB 4.9MB/s \n","\u001b[?25hInstalling collected packages: pyspellchecker\n","Successfully installed pyspellchecker-0.5.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHp2JkgGB606","executionInfo":{"status":"ok","timestamp":1610135550872,"user_tz":0,"elapsed":67335,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"472ff644-631f-4cf6-fa45-ea0a33803c57"},"source":["import csv\n","import time\n","import re\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from time import time\n","\n","from pycontractions import Contractions # For expanding contractions\n","\n","# For lemmatization\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","\n","# For removing stop words\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","\n","# For removing accents\n","import unicodedata \n","\n","# For removing emoticons\n","from emot.emo_unicode import EMOTICONS\n","\n","# For transforming the emojis into their description\n","import demoji\n","demoji.download_codes()\n","\n","# For spellchecking \n","from spellchecker import SpellChecker\n","\n","# Machine Learning Models\n","from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import svm\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","# Hypertuning\n","import itertools \n","import tensorflow as tf\n","from tensorboard.plugins.hparams import api as hp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","Downloading emoji data ...\n","... OK (Got response in 0.13 seconds)\n","Writing emoji data to /root/.demoji/codes.json ...\n","... OK\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l4h2AvFRCFTZ"},"source":["### Constants"]},{"cell_type":"code","metadata":{"id":"7XXbWR65BAAX"},"source":["# This dataset is preprocessed, the hashtag, mentions, urls and punctuation that is not emoticon or ' has been removed.\n","dataset_url = \"https://drive.google.com/uc?export=download&id=1wEAHS8-pzvKa7tIz99HJNSfKYwsEqs3T\"\n","contraction_expander = Contractions(api_key=\"glove-twitter-100\")\n","lemmatizer = WordNetLemmatizer()\n","\n","# Do not consider negative stop words as stop words or it will change sentiment\n","english_stop_words = set(stopwords.words('english')) \n","english_stop_words.remove('no')\n","english_stop_words.remove('nor')\n","english_stop_words.remove('not')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TwZ-6fy3skF"},"source":["# https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlhBECz_y7qJ"},"source":["## Preprocessing Methods"]},{"cell_type":"code","metadata":{"id":"nPmyDP5__qmb"},"source":["example_sentence = \"I've been to M√°laga and Alcorc√≥n to the world's most famous burger shop ‚ù§Ô∏è üòã I ain't flying ‚ò∫Ô∏èüëç :)\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mKyJVH-mTjDg"},"source":["### Emojis and emoticons\n","The core value of the research, the main hyper-tuning will be done by processing the data set, eliminating or maintaining each of these elements."]},{"cell_type":"code","metadata":{"id":"YOnWCkM1UK85"},"source":["emoji_pattern = re.compile(\"[\"\n","                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                u\"\\U00002702-\\U000027B0\"\n","                u\"\\U000024C2-\\U0001F251\"\n","                u\"\\U0001f926-\\U0001f937\"\n","                u'\\U00010000-\\U0010ffff'\n","                u\"\\u200d\"\n","                u\"\\u2640-\\u2642\"\n","                u\"\\u2600-\\u2B55\"\n","                u\"\\u23cf\"\n","                u\"\\u23e9\"\n","                u\"\\u231a\"\n","                u\"\\u3030\"\n","                u\"\\ufe0f\"\n","    \"]+\", flags=re.UNICODE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuL3XZY6TkaR"},"source":["def remove_emojis_text(text):\n","  return emoji_pattern.sub(r'', text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"BNZwT1JuVtO6","executionInfo":{"status":"ok","timestamp":1610135550876,"user_tz":0,"elapsed":67320,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"f07b351d-236b-4c72-a162-069e8a74b5e2"},"source":["remove_emojis_text(example_sentence)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I've been to M√°laga and Alcorc√≥n to the world's most famous burger shop   I ain't flying  :)\""]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"zM0WLqLBTtAj"},"source":["def remove_emoticons_text(text):\n","  emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n","  return emoticon_pattern.sub(r'', text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kTmq8GkDUfSd","executionInfo":{"status":"ok","timestamp":1610135550878,"user_tz":0,"elapsed":67312,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"9e40034a-1e40-4497-a87a-e55fe4c78494"},"source":["remove_emoticons_text(\"Hi :)\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Hi '"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"SAQ7YMOSTtQ0"},"source":["def remove_hashtags_text(text):\n","  return re.sub(r\"#(\\w+)\", ' ', text, flags=re.MULTILINE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"X1vusvS7YiaD","executionInfo":{"status":"ok","timestamp":1610135550879,"user_tz":0,"elapsed":67303,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"101c8564-0a11-4575-b51f-bf223a04e0ca"},"source":["remove_hashtags_text(\"Hi #friend\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Hi  '"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"UPOKI7NhX4t2"},"source":["def remove_mentions_text(text):\n","  return re.sub(r\"@(\\w+)\", ' ', text, flags=re.MULTILINE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dF8KqkYfy_Is"},"source":["### Remove accents from text\n","[Source](https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html)"]},{"cell_type":"code","metadata":{"id":"o3qo2-Xty9g1"},"source":["def remove_accents(text):\n","    unaccented_text = ''.join((char for char in unicodedata.normalize('NFD', text) if unicodedata.category(char) != 'Mn'))\n","    return unaccented_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ojYIb9dPzMm5","executionInfo":{"status":"ok","timestamp":1610135550880,"user_tz":0,"elapsed":67292,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"ca00b326-89a0-4468-f2ea-097800e1eaea"},"source":["# Make sure not emoticons nor emojis are removed\n","remove_accents(example_sentence)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I've been to Malaga and Alcorcon to the world's most famous burger shop ‚ù§ üòã I ain't flying ‚ò∫üëç :)\""]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"iD00JmjJ1txW"},"source":["### Expanding Contractions\n","In this step we remove the shorthed versions of words and syllabes, so it helps to the text standarization. Examples would be, do not to don‚Äôt and I would to I‚Äôd. [More info on why to use it](https://medium.com/@lukei_3514/dealing-with-contractions-in-nlp-d6174300876b)\n","\n","Using [pycontractions library](https://pypi.org/project/pycontractions/), using an advanced three-step approach."]},{"cell_type":"code","metadata":{"id":"D0Nwe2k516OY"},"source":["def expand_contractions(text):\n","  expanded_text = contraction_expander.expand_texts([text], precise=True) # Precise helps solving ain't ambuiguity\n","  return list(expanded_text)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"OlfynyPO5hFJ","executionInfo":{"status":"ok","timestamp":1610135778272,"user_tz":0,"elapsed":294673,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"abcacadf-fc94-4051-ffba-5538033ce734"},"source":["expand_contractions(example_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 387.1/387.1MB downloaded\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I have been to M√°laga and Alcorc√≥n to the world's most famous burger shop ‚ù§Ô∏è üòã I have not flying ‚ò∫Ô∏èüëç :)\""]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"-4GQ-4I38N9b"},"source":["### Lemmatization\n","This approach allows to extract the root forms of the words in the text, thus generating more occurrences of the same meaning of the word, which assists in the standardization of the text. Lemmatization was selected instead of stemming because speed is not a major concern in this case and the result is more representative for the type of text being used. [See differences](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)"]},{"cell_type":"code","metadata":{"id":"J3Yj__3J81cR"},"source":["def lemmatize_text(text):\n","  lemmatized_text = \" \".join([lemmatizer.lemmatize(word, pos=\"v\") for word in text.split(\" \")])\n","  return lemmatized_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"b_0IzIJN84aJ","executionInfo":{"status":"ok","timestamp":1610135780042,"user_tz":0,"elapsed":296433,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"55f4ee20-caf0-4af4-81f6-a5db7f9f1a67"},"source":["lemmatize_text(example_sentence)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I've be to M√°laga and Alcorc√≥n to the world's most famous burger shop ‚ù§Ô∏è üòã I ain't fly ‚ò∫Ô∏èüëç :)\""]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"yYfkE7kz_4nD"},"source":["### Removing Stopwords\n","Remove words with little significance in the text. Typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords are a, an, the, and the like."]},{"cell_type":"code","metadata":{"id":"6wZA3ZP7AA6z"},"source":["def remove_stop_words_text(text):\n","  word_tokens = word_tokenize(text) \n","  filtered_text = \" \".join([word for word in word_tokens if not word in english_stop_words])\n","  return filtered_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"uLQRRDloAmnI","executionInfo":{"status":"ok","timestamp":1610135780043,"user_tz":0,"elapsed":296424,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"90f35be5-fda0-4048-d651-5ffb46498043"},"source":["remove_stop_words_text(example_sentence)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I 've M√°laga Alcorc√≥n world 's famous burger shop ‚ù§Ô∏è üòã I ai n't flying ‚ò∫Ô∏èüëç : )\""]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"nBY2lIsg-imj"},"source":["### Transforming Emojis and Emoticons into text\n","[Source](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing#Conversion-of-Emoji-to-Words)"]},{"cell_type":"code","metadata":{"id":"zJXwJMw4fGlb"},"source":["def convert_emojis(text):\n","  emoticon_dic = demoji.findall(text)\n","  for emot, value in emoticon_dic.items():\n","    text = text.replace(emot, value.lower().replace(\" \", \"_\") + \" \")\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ylUuwdZdfOp8","executionInfo":{"status":"ok","timestamp":1610135780045,"user_tz":0,"elapsed":296416,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"122e5128-88dc-4bae-df9e-e43574aafc92"},"source":["convert_emojis(example_sentence)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I've been to M√°laga and Alcorc√≥n to the world's most famous burger shop red_heart  face_savoring_food  I ain't flying smiling_face thumbs_up  :)\""]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"7a9KlT628P-e"},"source":["def convert_emoticons(text):\n","  for emot in EMOTICONS:\n","    value = EMOTICONS[emot].lower().replace(\",\", \"\").split()\n","    text = re.sub(u'(' + emot + ')', \"_\".join(value) + \" \", text)\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bZ4SpkpV8Trf","executionInfo":{"status":"ok","timestamp":1610135780046,"user_tz":0,"elapsed":296407,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"98b85686-fcb2-4c45-d503-3e2297aa202b"},"source":["convert_emoticons(example_sentence)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I've been to M√°laga and Alcorc√≥n to the world's most famous burger shop ‚ù§Ô∏è üòã I ain't flying ‚ò∫Ô∏èüëç happy_face_or_smiley \""]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"nAyPWeW1-zQh","executionInfo":{"status":"ok","timestamp":1610135780046,"user_tz":0,"elapsed":296399,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"9bf758ef-131d-4d5a-af83-ef63562e60dd"},"source":["convert_emoticons(convert_emojis(example_sentence))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I've been to M√°laga and Alcorc√≥n to the world's most famous burger shop red_heart  face_savoring_food  I ain't flying smiling_face thumbs_up  happy_face_or_smiley \""]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"r3OJw_XVJKgK"},"source":["### Spelling check\n","Note: Discarded in preprocessing because of poor results"]},{"cell_type":"code","metadata":{"id":"Tatk1wc6JMpp"},"source":["spell = SpellChecker()\n","def correct_spellings(text):\n","    corrected_text = []\n","    misspelled_words = spell.unknown(text.split())\n","    for word in text.split():\n","        if word in misspelled_words:\n","            corrected_text.append(spell.correction(word))\n","        else:\n","            corrected_text.append(word)\n","    return \" \".join(corrected_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oQWMNcK202Ml","executionInfo":{"status":"ok","timestamp":1610135780267,"user_tz":0,"elapsed":296610,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"5c558f35-526c-4714-f8a0-5fc81953c9ba"},"source":["correct_spellings('I\\'ve bee to M√°laga and Alcorc√≥n to the world most famous burge shop')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I've bee to M√°laga and Alcorc√≥n to the world most famous urge shop\""]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"7KTAooGRBG2J"},"source":["### Preprocess Wrapper\n","Method that unifies previous transformations into a single method"]},{"cell_type":"code","metadata":{"id":"DLdR2C5NBOXO"},"source":["def preprocess_text(text, \n","                    remove_emojis = False, \n","                    remove_emoticons = False, \n","                    remove_hashtags = True, \n","                    remove_mentions = False, \n","                    remove_stop_words = True,\n","                    transform_lowercase = True, \n","                    transform_emojis_text = False, \n","                    transform_emoticons_text = False,\n","                    transform_lemmatize = True):\n","  preprocessed_text = text\n","  preprocessed_text = remove_accents(preprocessed_text)\n","  preprocessed_text = expand_contractions(preprocessed_text)\n","\n","  # Elements removal\n","  if remove_emojis:\n","    preprocessed_text = remove_emojis_text(preprocessed_text)\n","  if remove_emoticons:\n","    preprocessed_text = remove_emoticons_text(preprocessed_text)\n","  if remove_hashtags:\n","    preprocessed_text = remove_hashtags_text(preprocessed_text)\n","  if remove_mentions:\n","    preprocessed_text = remove_mentions_text(preprocessed_text)\n","  if remove_stop_words:\n","    preprocessed_text = remove_stop_words_text(preprocessed_text)\n","\n","  # Transformations\n","  if transform_lowercase:\n","    preprocessed_text = preprocessed_text.lower()\n","  if transform_emojis_text and not remove_emojis:\n","    preprocessed_text = convert_emojis(preprocessed_text)\n","  if transform_emoticons_text and not remove_emoticons:\n","    preprocessed_text = convert_emoticons(preprocessed_text)\n","  if transform_lemmatize:\n","    preprocessed_text = lemmatize_text(preprocessed_text)\n","\n","  # Remove 's if there is any left\n","  preprocessed_text = re.sub(r'\\'s', '', preprocessed_text)\n","  preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text)\n","\n","  return preprocessed_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"O7BYqlpIBhqR","executionInfo":{"status":"ok","timestamp":1610135780798,"user_tz":0,"elapsed":297130,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"a1b30d65-11be-41ea-ed69-ed6387960e04"},"source":["preprocess_text(example_sentence, remove_emoticons=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'i malaga alcorcon world famous burger shop ‚ù§ üòã i not fly ‚ò∫üëç'"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"LJO4TPOPBCa3"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"z2HD20p_BI5J"},"source":["### Initialization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"LKmOwQLECNdO","executionInfo":{"status":"ok","timestamp":1610135781899,"user_tz":0,"elapsed":298223,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"b94909c1-2526-4a24-d9b9-2debf725bf15"},"source":["df = pd.read_csv(dataset_url)\n","df.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sentiment</th>\n","      <th>sentiment_confidence</th>\n","      <th>text</th>\n","      <th>sentiment_numeric</th>\n","      <th>hasEmoji</th>\n","      <th>hasEmoticon</th>\n","      <th>hasUrl</th>\n","      <th>hasMention</th>\n","      <th>hasHashtag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>positive</td>\n","      <td>1.0000</td>\n","      <td>@JetBlue incredible PR team  üëèüëèüëèüëè</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>@SouthwestAir can you please DM me  I have a q...</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>neutral</td>\n","      <td>0.6733</td>\n","      <td>@SouthwestAir how oh how do we get tickets</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@united and now your rep just hung up on me af...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@AmericanAir our flight was Cancelled Flightle...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>neutral</td>\n","      <td>0.6690</td>\n","      <td>@SouthwestAir @travelportland welcome to Portl...</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>positive</td>\n","      <td>0.6701</td>\n","      <td>@VirginAmerica I m looking forward to watching...</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>positive</td>\n","      <td>0.3586</td>\n","      <td>@united Just sent  Thanks :)</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@USAirways I tried to call your customer servi...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>positive</td>\n","      <td>0.6560</td>\n","      <td>@united Hubby made it by the skin of his teeth...</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>neutral</td>\n","      <td>0.6809</td>\n","      <td>@USAirways I think it's ok</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>@JetBlue  Our fleet's on fleek  C'mon famüò≠üò≠ j...</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@USAirways #stuck in Florida with no flights  ...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@AmericanAir that luggage you forgot #mia he j...</td>\n","      <td>-1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@united it's like you're trying to make me hat...</td>\n","      <td>-1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>positive</td>\n","      <td>1.0000</td>\n","      <td>@AmericanAir thank you</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>negative</td>\n","      <td>0.6801</td>\n","      <td>@VirginAmerica I'm pulling my hair out trying ...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>negative</td>\n","      <td>0.6406</td>\n","      <td>@JetBlue  Our fleet's on fleek</td>\n","      <td>-1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>positive</td>\n","      <td>1.0000</td>\n","      <td>@SouthwestAir Scott is the best  Thank yo from...</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>positive</td>\n","      <td>1.0000</td>\n","      <td>@AmericanAir Such a suprise  New vanity kit se...</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Unnamed: 0 sentiment  sentiment_confidence  ... hasUrl  hasMention  hasHashtag\n","0            1  positive                1.0000  ...  False        True       False\n","1            2   neutral                1.0000  ...  False        True       False\n","2            3   neutral                0.6733  ...  False        True       False\n","3            4  negative                1.0000  ...  False        True       False\n","4            5  negative                1.0000  ...  False        True        True\n","5            6   neutral                0.6690  ...   True        True       False\n","6            7  positive                0.6701  ...  False        True       False\n","7            8  positive                0.3586  ...  False        True       False\n","8            9  negative                1.0000  ...  False        True       False\n","9           10  positive                0.6560  ...  False        True       False\n","10          11   neutral                0.6809  ...  False        True       False\n","11          12   neutral                1.0000  ...   True        True       False\n","12          13  negative                1.0000  ...  False        True        True\n","13          14  negative                1.0000  ...  False        True        True\n","14          15  negative                1.0000  ...  False        True       False\n","15          16  positive                1.0000  ...  False        True       False\n","16          17  negative                0.6801  ...   True        True        True\n","17          18  negative                0.6406  ...   True        True        True\n","18          19  positive                1.0000  ...  False        True        True\n","19          20  positive                1.0000  ...   True        True       False\n","\n","[20 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"brYrbNDDPBjg"},"source":["### Processing"]},{"cell_type":"markdown","metadata":{"id":"HtzuGe0GJwTS"},"source":["Only keep texts with confidence > 0.6"]},{"cell_type":"code","metadata":{"id":"E1RNDpQcJ0LG"},"source":["df = df[df['sentiment_confidence'] > 0.6]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuGuGOCNMtwY"},"source":["Add other label transforming neutral as positive to have a better distinction (it will be used in hypertuning)"]},{"cell_type":"code","metadata":{"id":"C32q-jYjMwwW"},"source":["df['sentiment_binary'] = df['sentiment_numeric'].apply(lambda x: x + 1 if x != 1 else x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NbrQHb3vDUBx"},"source":["Preprocess texts"]},{"cell_type":"code","metadata":{"id":"_zscKaTeCekB"},"source":["df['text_clean'] = df['text'].apply(lambda text: preprocess_text(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zxZLzc8NOrBg"},"source":["df['text_clean_no_graphics'] = df['text_clean'].apply(lambda text: re.sub(r\"[^a-zA-Z0-9#@]+\", ' ', text))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eNHjBxhvJ5EA"},"source":["Show result"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SsmjLDJCJ599","executionInfo":{"status":"ok","timestamp":1610135793655,"user_tz":0,"elapsed":309963,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"7b0076d3-5b39-4472-d101-d68de43225fd"},"source":["df.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sentiment</th>\n","      <th>sentiment_confidence</th>\n","      <th>text</th>\n","      <th>sentiment_numeric</th>\n","      <th>hasEmoji</th>\n","      <th>hasEmoticon</th>\n","      <th>hasUrl</th>\n","      <th>hasMention</th>\n","      <th>hasHashtag</th>\n","      <th>sentiment_binary</th>\n","      <th>text_clean</th>\n","      <th>text_clean_no_graphics</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>positive</td>\n","      <td>1.0000</td>\n","      <td>@JetBlue incredible PR team  üëèüëèüëèüëè</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ jetblue incredible pr team üëèüëèüëèüëè</td>\n","      <td>@ jetblue incredible pr team</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>@SouthwestAir can you please DM me  I have a q...</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ southwestair please dm i question : )</td>\n","      <td>@ southwestair please dm i question</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>neutral</td>\n","      <td>0.6733</td>\n","      <td>@SouthwestAir how oh how do we get tickets</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ southwestair oh get ticket</td>\n","      <td>@ southwestair oh get ticket</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@united and now your rep just hung up on me af...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>@ unite rep hang 35 mins hold i ask supervisor...</td>\n","      <td>@ unite rep hang 35 mins hold i ask supervisor...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@AmericanAir our flight was Cancelled Flightle...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>@ americanair flight cancel flightled rebooked...</td>\n","      <td>@ americanair flight cancel flightled rebooked...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>neutral</td>\n","      <td>0.6690</td>\n","      <td>@SouthwestAir @travelportland welcome to Portl...</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ southwestair @ travelportland welcome portla...</td>\n","      <td>@ southwestair @ travelportland welcome portla...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>positive</td>\n","      <td>0.6701</td>\n","      <td>@VirginAmerica I m looking forward to watching...</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ virginamerica i look forward watch oscars fl...</td>\n","      <td>@ virginamerica i look forward watch oscars fl...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@USAirways I tried to call your customer servi...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>@ usairways i try call customer service line k...</td>\n","      <td>@ usairways i try call customer service line k...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>positive</td>\n","      <td>0.6560</td>\n","      <td>@united Hubby made it by the skin of his teeth...</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ unite hubby make skin teeth : )</td>\n","      <td>@ unite hubby make skin teeth</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>neutral</td>\n","      <td>0.6809</td>\n","      <td>@USAirways I think it's ok</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ usairways i think ok</td>\n","      <td>@ usairways i think ok</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>@JetBlue  Our fleet's on fleek  C'mon famüò≠üò≠ j...</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ jetblue our fleet fleek c'mon famüò≠üò≠ no ok</td>\n","      <td>@ jetblue our fleet fleek c mon fam no ok</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@USAirways #stuck in Florida with no flights  ...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>@ usairways florida no flight service suck</td>\n","      <td>@ usairways florida no flight service suck</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@AmericanAir that luggage you forgot #mia he j...</td>\n","      <td>-1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>@ americanair luggage forget oscarüòÑüíùüíùüíù</td>\n","      <td>@ americanair luggage forget oscar</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>negative</td>\n","      <td>1.0000</td>\n","      <td>@united it's like you're trying to make me hat...</td>\n","      <td>-1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>@ unite like try make hate airline fee check b...</td>\n","      <td>@ unite like try make hate airline fee check b...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>positive</td>\n","      <td>1.0000</td>\n","      <td>@AmericanAir thank you</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ americanair thank</td>\n","      <td>@ americanair thank</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>negative</td>\n","      <td>0.6801</td>\n","      <td>@VirginAmerica I'm pulling my hair out trying ...</td>\n","      <td>-1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>@ virginamerica i pull hair try book flight yo...</td>\n","      <td>@ virginamerica i pull hair try book flight yo...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>negative</td>\n","      <td>0.6406</td>\n","      <td>@JetBlue  Our fleet's on fleek</td>\n","      <td>-1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>@ jetblue our fleet fleek</td>\n","      <td>@ jetblue our fleet fleek</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>positive</td>\n","      <td>1.0000</td>\n","      <td>@SouthwestAir Scott is the best  Thank yo from...</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>1</td>\n","      <td>@ southwestair scott best thank yo bottom hear...</td>\n","      <td>@ southwestair scott best thank yo bottom hear...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>positive</td>\n","      <td>1.0000</td>\n","      <td>@AmericanAir Such a suprise  New vanity kit se...</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ americanair such suprise new vanity kit set ...</td>\n","      <td>@ americanair such suprise new vanity kit set ...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>21</td>\n","      <td>neutral</td>\n","      <td>1.0000</td>\n","      <td>@SouthwestAir  Wife's RR pts expired a few day...</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>@ southwestair wife rr pts expire days ago pla...</td>\n","      <td>@ southwestair wife rr pts expire days ago pla...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Unnamed: 0  ...                             text_clean_no_graphics\n","0            1  ...                      @ jetblue incredible pr team \n","1            2  ...               @ southwestair please dm i question \n","2            3  ...                       @ southwestair oh get ticket\n","3            4  ...  @ unite rep hang 35 mins hold i ask supervisor...\n","4            5  ...  @ americanair flight cancel flightled rebooked...\n","5            6  ...  @ southwestair @ travelportland welcome portla...\n","6            7  ...  @ virginamerica i look forward watch oscars fl...\n","8            9  ...  @ usairways i try call customer service line k...\n","9           10  ...                     @ unite hubby make skin teeth \n","10          11  ...                             @ usairways i think ok\n","11          12  ...          @ jetblue our fleet fleek c mon fam no ok\n","12          13  ...         @ usairways florida no flight service suck\n","13          14  ...                @ americanair luggage forget oscar \n","14          15  ...  @ unite like try make hate airline fee check b...\n","15          16  ...                                @ americanair thank\n","16          17  ...  @ virginamerica i pull hair try book flight yo...\n","17          18  ...                          @ jetblue our fleet fleek\n","18          19  ...  @ southwestair scott best thank yo bottom hear...\n","19          20  ...  @ americanair such suprise new vanity kit set ...\n","20          21  ...  @ southwestair wife rr pts expire days ago pla...\n","\n","[20 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pkoHfubN24z","executionInfo":{"status":"ok","timestamp":1610135793656,"user_tz":0,"elapsed":309956,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"74ed89d6-4f7b-408b-aa8f-aa734a221676"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1294, 13)"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"i0r4lb4TvDQo"},"source":["### Data preparation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBRlKJXExkOy","executionInfo":{"status":"ok","timestamp":1610135793656,"user_tz":0,"elapsed":309948,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"e2ab9251-e485-4d9c-8a5e-4e1039b789bb"},"source":["# For starting only use train_test split. Future: Cross validation \n","sub_df = df.loc[:, ['sentiment_numeric', 'sentiment_binary', 'text_clean']]\n","\n","train, test = train_test_split(sub_df, test_size=0.1, random_state=1)\n","\n","# Prepare input data for the algorithms\n","X_train = train['text_clean'].values\n","X_test = test['text_clean'].values\n","sentences = np.append(X_train, X_test)\n","\n","Y_train = train['sentiment_binary']\n","Y_test = test['sentiment_binary']\n","\n","print(X_train)\n","print(Y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['@ americanair @ airport 8hrs tell nothing anyone rep say wait mechanics finish lunch'\n"," '@ jetblue tweet get word üò¢'\n"," '@ unite appreciate sentiment able get grind still miss connection' ...\n"," '@ jetblue i nervous sunday be flight baltimore boston any suggestions i need boston monday'\n"," '@ unite cat flight delay 1+hour arrive hawaii 5 amp i not able pick tomorrow üò≠'\n"," '@ usairways thank make miss meet dallas 700 dollars toilet üòä']\n","1010    0\n","435     0\n","133     1\n","316     0\n","465     1\n","       ..\n","728     1\n","922     0\n","1118    1\n","240     0\n","1082    0\n","Name: sentiment_binary, Length: 1164, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z1_7QbhzVTjo"},"source":["### Vectorizers\n","Two vectorizers will be used in the hypertuning:\n","- Count: This will transform the text in our data frame into a **bag of words model**, which will contain a sparse matrix of integers. The number of occurrences of each word will be counted and printed.\n","- TF-IDF: Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n","\n","We will need to convert the text into a bag-of-words model since the logistic regression algorithm cannot understand text."]},{"cell_type":"code","metadata":{"id":"ugp14JLNUWuW"},"source":["def get_train_test_vectors(vectorizer_name, X_train, X_test):\n","  vectorizer = None      \n","  if vectorizer_name == \"count\":\n","    vectorizer = CountVectorizer(\n","        analyzer = 'word',\n","        ngram_range=(1, 2), # Using both unigrams and bigrams\n","        token_pattern=r'[^\\s]+') # To allow emojis and emoticons\n","  elif vectorizer_name == 'tf-idf':\n","    vectorizer = TfidfVectorizer(\n","        analyzer = 'word',\n","        ngram_range=(1, 2), # Using both unigrams and bigrams\n","        token_pattern=r'[^\\s]+') # To allow emojis and emoticons\n","    \n","  vectorizer.fit(sentences)\n","\n","  train_vectors = vectorizer.transform(X_train)\n","  test_vectors = vectorizer.transform(X_test)\n","\n","  return train_vectors, test_vectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pY8CR7hrDJI6"},"source":["## Algorithms"]},{"cell_type":"code","metadata":{"id":"c9H0fb2WOqx6"},"source":["train_vectors, test_vectors = get_train_test_vectors(\"count\", X_train, X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0YDOJl-fbwp"},"source":["### Support Vector Machine (SVM)"]},{"cell_type":"code","metadata":{"id":"pmC3rCMABbUk"},"source":["def get_svm_metrics(train_vectors, train_labels, test_vectors, test_labels, regularization, kernel, gamma):\n","  start_time = time()\n","  # Perform classification with SVM\n","  svm_classifier = svm.SVC(C = regularization, kernel = kernel, gamma = gamma, probability = True)\n","  # Train Model\n","  svm_classifier.fit(train_vectors, train_labels)\n","\n","  # Predict Model\n","  svm_classifier_prediction = svm_classifier.predict(test_vectors)\n","  svm_classifier_prediction_proba = svm_classifier.predict_proba(test_vectors)\n","\n","  # Convert probability to class for roc_auc_score in case of binary\n","  if svm_classifier_prediction_proba.shape[1] == 2: \n","    svm_classifier_prediction_proba = np.array([np.argmax(x) for x in svm_classifier_prediction_proba])\n","\n","  # Report metrics\n","  accuracy = accuracy_score(test_labels, svm_classifier_prediction)\n","  f1 = f1_score(test_labels, svm_classifier_prediction, average = 'weighted')\n","  auc = roc_auc_score(test_labels, svm_classifier_prediction_proba, average = 'weighted', multi_class = 'ovr')\n","  end_time = time() - start_time\n","\n","  return accuracy, f1, auc, end_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UNFxJE1fZ4N","executionInfo":{"status":"ok","timestamp":1610135794998,"user_tz":0,"elapsed":311277,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"ac1a9a68-b7be-41df-ff58-dd91583238b1"},"source":["get_svm_metrics(train_vectors, Y_train, test_vectors, Y_test, 1.0, 'linear', 'auto')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.8, 0.8009696969696971, 0.7793432982112228, 1.2597923278808594)"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"lfjvFsvFTvb3"},"source":["### Multinomial Naive Bayes (MNB)"]},{"cell_type":"code","metadata":{"id":"u1xwdwpNDdj1"},"source":["def get_mnb_metrics(train_vectors, train_labels, test_vectors, test_labels, alpha, fit_prior):\n","  start_time = time()\n","  # Perform classification with SVM\n","  mnb_classifier = MultinomialNB(alpha = alpha, fit_prior = fit_prior)\n","  # Train Model\n","  mnb_classifier.fit(train_vectors, train_labels)\n","  # Predict Model\n","  mnb_classifier_prediction = mnb_classifier.predict(test_vectors)\n","  mnb_classifier_prediction_proba = mnb_classifier.predict_proba(test_vectors)\n","\n","  # Convert probability to class for roc_auc_score in case of binary\n","  if mnb_classifier_prediction_proba.shape[1] == 2: \n","    mnb_classifier_prediction_proba = np.array([np.argmax(x) for x in mnb_classifier_prediction_proba])\n","\n","  # Report metrics\n","  accuracy = accuracy_score(test_labels, mnb_classifier_prediction)\n","  f1 = f1_score(test_labels, mnb_classifier_prediction, average = 'weighted')\n","  auc = roc_auc_score(test_labels, mnb_classifier_prediction_proba, average = 'weighted', multi_class = 'ovr')\n","  end_time = time() - start_time\n","\n","  return accuracy, f1, auc, end_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTT9x8iEZNBN","executionInfo":{"status":"ok","timestamp":1610135794999,"user_tz":0,"elapsed":311267,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"6513df44-b1d3-451e-c611-a74e6857a4c5"},"source":["get_mnb_metrics(train_vectors, Y_train, test_vectors, Y_test, 1.0, False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.7923076923076923,\n"," 0.7940828402366864,\n"," 0.8011516785101691,\n"," 0.00892186164855957)"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"4jyNh2X2YEMk"},"source":["### K-Nearest Neighbors (KNN)\n"]},{"cell_type":"code","metadata":{"id":"E_rkhD-4FdXC"},"source":["def get_knn_metrics(train_vectors, train_labels, test_vectors, test_labels, n_neighbors, weights):\n","  start_time = time()\n","  # Perform classification with SVM\n","  knn_classifier = KNeighborsClassifier(n_neighbors = n_neighbors, weights = weights)\n","  # Train Model\n","  knn_classifier.fit(train_vectors, train_labels)\n","  # Predict Model\n","  knn_classifier_prediction = knn_classifier.predict(test_vectors)\n","  knn_classifier_prediction_proba = knn_classifier.predict_proba(test_vectors)\n","\n","  # Convert probability to class for roc_auc_score in case of binary\n","  if knn_classifier_prediction_proba.shape[1] == 2: \n","    knn_classifier_prediction_proba = np.array([np.argmax(x) for x in knn_classifier_prediction_proba])\n","\n","  # Report metrics\n","  accuracy = accuracy_score(test_labels, knn_classifier_prediction)\n","  f1 = f1_score(test_labels, knn_classifier_prediction, average = 'weighted')\n","  auc = roc_auc_score(test_labels, knn_classifier_prediction_proba, average = 'weighted', multi_class = 'ovr')\n","  end_time = time() - start_time\n","\n","  return accuracy, f1, auc, end_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x31joXQDYTrZ","executionInfo":{"status":"ok","timestamp":1610135795000,"user_tz":0,"elapsed":311259,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"a88d8c21-d41d-4f17-a4b3-a5e98ac673f9"},"source":["get_knn_metrics(train_vectors, Y_train, test_vectors, Y_test, n_neighbors=3, weights='distance')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.5846153846153846,\n"," 0.4731538461538462,\n"," 0.5023278608184268,\n"," 0.02318739891052246)"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"mGjaT0D2hQOZ"},"source":["### Decision Tree (DT)"]},{"cell_type":"code","metadata":{"id":"94iB0OjjhPMk"},"source":["def get_dt_metrics(train_vectors, train_labels, test_vectors, test_labels, max_depth = None, max_features = None, criterion = \"gini\"):\n","  start_time = time()\n","  # Perform classification with RF\n","  dt_classifier = DecisionTreeClassifier(max_depth = max_depth, max_features = max_features, criterion = criterion)\n","  # Train Model\n","  dt_classifier.fit(train_vectors, train_labels)\n","  # Predict Model\n","  dt_classifier_prediction = dt_classifier.predict(test_vectors)\n","  dt_classifier_prediction_proba = dt_classifier.predict_proba(test_vectors)\n","\n","  # Convert probability to class for roc_auc_score in case of binary\n","  if dt_classifier_prediction_proba.shape[1] == 2: \n","    dt_classifier_prediction_proba = np.array([np.argmax(x) for x in dt_classifier_prediction_proba])\n","\n","  # Report metrics\n","  accuracy = accuracy_score(test_labels, dt_classifier_prediction)\n","  f1 = f1_score(test_labels, dt_classifier_prediction, average = 'weighted')\n","  auc = roc_auc_score(test_labels, dt_classifier_prediction_proba, average = 'weighted', multi_class = 'ovr')\n","  end_time = time() - start_time\n","\n","  return accuracy, f1, auc, end_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsuGaMY2iaxm","executionInfo":{"status":"ok","timestamp":1610135795385,"user_tz":0,"elapsed":311635,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"e9d22648-7e59-4be2-e4c2-6704c2b70122"},"source":["get_dt_metrics(train_vectors, Y_train, test_vectors, Y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.7846153846153846,\n"," 0.7862717911744588,\n"," 0.7887772604753738,\n"," 0.14447522163391113)"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"5EojsVZab6TN"},"source":["### Random Forest (RF)"]},{"cell_type":"code","metadata":{"id":"jUhKiHSrb-KV"},"source":["def get_rf_metrics(train_vectors, train_labels, test_vectors, test_labels, n_trees = 100, max_depth = None, max_features = \"auto\", criterion = \"gini\"):\n","  start_time = time()\n","  # Perform classification with RF\n","  rf_classifier = RandomForestClassifier(n_estimators = n_trees, max_depth = max_depth, max_features = max_features, criterion = criterion)\n","  # Train Model\n","  rf_classifier.fit(train_vectors, train_labels)\n","  # Predict Model\n","  rf_classifier_prediction = rf_classifier.predict(test_vectors)\n","  rf_classifier_prediction_proba = rf_classifier.predict_proba(test_vectors)\n","\n","  # Convert probability to class for roc_auc_score in case of binary\n","  if rf_classifier_prediction_proba.shape[1] == 2: \n","    rf_classifier_prediction_proba = np.array([np.argmax(x) for x in rf_classifier_prediction_proba])\n","\n","  # Report metrics\n","  accuracy = accuracy_score(test_labels, rf_classifier_prediction)\n","  f1 = f1_score(test_labels, rf_classifier_prediction, average = 'weighted')\n","  auc = roc_auc_score(test_labels, rf_classifier_prediction_proba, average = 'weighted', multi_class = 'ovr')\n","  end_time = time() - start_time\n","\n","  return accuracy, f1, auc, end_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CXmR17Cgf7f","executionInfo":{"status":"ok","timestamp":1610135795387,"user_tz":0,"elapsed":311627,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"9507a67f-73b7-4d70-aa94-d4bb6b7d9034"},"source":["get_rf_metrics(train_vectors, Y_train, test_vectors, Y_test, n_trees = 10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.7846153846153846,\n"," 0.7831185443992592,\n"," 0.771134525851507,\n"," 0.16304922103881836)"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"PhhBsoyCGT6S"},"source":["### Deep Neural Network (DNN)\n","*Discarded because of poor accuracy (probably because of small amount of data)*"]},{"cell_type":"code","metadata":{"id":"CWheodTZGY4g"},"source":["def get_dnn_metrics(train_vectors, train_labels, test_vectors, test_labels, activation, optimizer):\n","  dnn_model = Sequential([\n","    Dense(25, activation=activation),\n","    Dense(10, activation=activation),\n","    Dense(1, activation='sigmoid')\n","  ])\n","  # Compile the model  \n","  dnn_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) # ‚Äòsparse_categorical_crossentropy‚Äò for multi-class classification.\n","\n","  # Fit the model\n","  dnn_model.fit(train_vectors, train_labels, epochs=100, batch_size=16, verbose=0)\n","\n","  # Output accuracy\n","  return dnn_model.evaluate(test_vectors, test_labels, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwaNMkm5HyvF","executionInfo":{"status":"ok","timestamp":1610135818684,"user_tz":0,"elapsed":334915,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"92eeaebf-7652-4199-8ba8-a16d0d9174c2"},"source":["get_dnn_metrics(train_vectors.toarray(), Y_train, test_vectors.toarray(), Y_test, activation='relu', optimizer='sgd')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.7176179885864258, 0.800000011920929]"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"TuFLM45WBTdo"},"source":["## Hypertuning"]},{"cell_type":"code","metadata":{"id":"eb-ZqKBuBU5U"},"source":["# General\n","METRIC_ACCURACY = 'accuracy'\n","METRIC_F1_SCORE = 'f1-score'\n","METRIC_AUC = 'auc'\n","METRIC_TIME = 'time'\n","\n","# For sentiment label to use\n","HP_SENTIMENT_TYPE = hp.HParam('sentiment_type', hp.Discrete(['sentiment_binary', 'sentiment_numeric'])) # Discarded after preselection: 'sentiment_numeric'\n","\n","# For model\n","HP_MODEL = hp.HParam('model', hp.Discrete(['svm', 'mnb'])) #, 'knn', 'dt', 'rf']))\n","HP_VECTORIZER = hp.HParam('vectorizer', hp.Discrete(['tf-idf', 'count']))\n","\n","# For Data processing\n","HP_EMOJI_INCLUDED = hp.HParam('emoji_included', hp.Discrete([True, False]))\n","HP_EMOTICON_INCLUDED = hp.HParam('emoticon_included', hp.Discrete([True, False]))\n","HP_TRANSFORM_EMOJIS_TEXT = hp.HParam('transform_emojis_text', hp.Discrete([True, False]))\n","HP_TRANSFORM_EMOTICONS_TEXT = hp.HParam('transform_emoticons_text', hp.Discrete([True, False]))\n","\n","HP_MENTION_INCLUDED = hp.HParam('mention_included', hp.Discrete([True, False])) # Discarded after preselection: False\n","HP_HASHTAG_INCLUDED = hp.HParam('hashtag_included', hp.Discrete([True, False])) # Discarded after preselection: True\n","HP_STOPWORDS_INCLUDED = hp.HParam('stopwords_included', hp.Discrete([True, False])) # Discarded after preselection: True\n","HP_TRANSFORM_LOWERCASE = hp.HParam('transform_lowercase', hp.Discrete([True, False])) # Discarded after preselection: False\n","\n","hparam_grid = {\n","  HP_MODEL: HP_MODEL.domain.values,\n","  HP_VECTORIZER: HP_VECTORIZER.domain.values,\n","  HP_EMOJI_INCLUDED: HP_EMOJI_INCLUDED.domain.values,\n","  HP_EMOTICON_INCLUDED: HP_EMOTICON_INCLUDED.domain.values,\n","  HP_TRANSFORM_EMOJIS_TEXT: HP_TRANSFORM_EMOJIS_TEXT.domain.values,\n","  HP_TRANSFORM_EMOTICONS_TEXT: HP_TRANSFORM_EMOTICONS_TEXT.domain.values,\n","  HP_MENTION_INCLUDED: HP_MENTION_INCLUDED.domain.values,\n","  HP_HASHTAG_INCLUDED: HP_HASHTAG_INCLUDED.domain.values,\n","  HP_STOPWORDS_INCLUDED: HP_STOPWORDS_INCLUDED.domain.values,\n","  HP_TRANSFORM_LOWERCASE: HP_TRANSFORM_LOWERCASE.domain.values,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtYVwF-uVXEJ"},"source":["### Algorithms Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"wa7ybXAkVaMy"},"source":["##### SVM"]},{"cell_type":"code","metadata":{"id":"Du9XaHYoJpO2"},"source":["HP_MODEL_SVM_C = hp.HParam('svm_regularization', hp.IntInterval(1, 3)) # Best after hypertuning => (1, 3)\n","HP_MODEL_SVM_KERNEL = hp.HParam('svm_kernel', hp.Discrete(['linear', 'rbf'])) # , 'sigmoid'])) # Best after hypertuning => rbf\n","HP_MODEL_SVM_GAMMA = hp.HParam('svm_gamma', hp.Discrete(['scale'])) # , 'auto'])) # Best after hypertuning => scale\n","\n","svm_hparam_grid = {\n","  HP_MODEL_SVM_C: range(HP_MODEL_SVM_C.domain.min_value, HP_MODEL_SVM_C.domain.max_value),\n","  HP_MODEL_SVM_KERNEL: HP_MODEL_SVM_KERNEL.domain.values,\n","  HP_MODEL_SVM_GAMMA: HP_MODEL_SVM_GAMMA.domain.values,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEJ_cGW5uxR9"},"source":["def hypertune_svm(session_num, train_vectors, y_train, test_vectors, y_test, hparams):\n","  for svm_hparams in list(create_tfparam_grid(svm_hparam_grid)):\n","    hparams.update(svm_hparams)\n","\n","    run_name = \"run-%d\" % session_num\n","    print('--- Starting trial: %s' % run_name)\n","    print({h.name: hparams[h] for h in hparams})\n","\n","    # Log to tensorboard\n","    with tf.summary.create_file_writer(run_dir + run_name).as_default():\n","      # Record the values used in this trial\n","      hp.hparams(hparams)  \n","      # Get metrics\n","      accuracy, f1, auc, time = get_svm_metrics(train_vectors, \n","                                  y_train, \n","                                  test_vectors, \n","                                  y_test, \n","                                  svm_hparams[HP_MODEL_SVM_C], \n","                                  svm_hparams[HP_MODEL_SVM_KERNEL],\n","                                  svm_hparams[HP_MODEL_SVM_GAMMA])\n","      tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n","      tf.summary.scalar(METRIC_F1_SCORE, f1, step=1)\n","      tf.summary.scalar(METRIC_AUC, auc, step = 1)\n","      tf.summary.scalar(METRIC_TIME, time, step = 1)\n","    session_num += 1\n","  return session_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"edfYFY1gVcYF"},"source":["#### MNB"]},{"cell_type":"code","metadata":{"id":"LlXt57VPLd1h"},"source":["# For MNB\n","HP_MODEL_MNB_ALPHA = hp.HParam('mnb_alpha', hp.IntInterval(1, 3)) # Best after hypertuning => (1, 3)\n","HP_MODEL_MNB_PRIOR = hp.HParam('mnb_fit_prior', hp.Discrete([True, False])) # Best after hypertuning => Not clear\n","\n","mnb_hparam_grid = {\n","  HP_MODEL_MNB_ALPHA: range(HP_MODEL_MNB_ALPHA.domain.min_value, HP_MODEL_MNB_ALPHA.domain.max_value),\n","  HP_MODEL_MNB_PRIOR: HP_MODEL_MNB_PRIOR.domain.values\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RbeQBBPtwI57"},"source":["def hypertune_mnb(session_num, train_vectors, y_train, test_vectors, y_test, hparams):\n","  for mnb_hparams in list(create_tfparam_grid(mnb_hparam_grid)):\n","    hparams.update(mnb_hparams)\n","\n","    run_name = \"run-%d\" % session_num\n","    print('--- Starting trial: %s' % run_name)\n","    print({h.name: hparams[h] for h in hparams})\n","\n","    # Log to tensorboard\n","    with tf.summary.create_file_writer(run_dir + run_name).as_default():\n","      # Record the values used in this trial\n","      hp.hparams(hparams)  \n","      # Get metrics\n","      accuracy, f1, auc, time = get_mnb_metrics(train_vectors, \n","                                  y_train, \n","                                  test_vectors, \n","                                  y_test,\n","                                  mnb_hparams[HP_MODEL_MNB_ALPHA], \n","                                  mnb_hparams[HP_MODEL_MNB_PRIOR])\n","      tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n","      tf.summary.scalar(METRIC_F1_SCORE, f1, step=1)\n","      tf.summary.scalar(METRIC_AUC, auc, step = 1)\n","      tf.summary.scalar(METRIC_TIME, time, step = 1)\n","    session_num += 1\n","\n","  return session_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQuc0SeJVgI9"},"source":["#### KNN"]},{"cell_type":"code","metadata":{"id":"XA-aj1NQLepK"},"source":["# For KNN\n","HP_MODEL_KNN_NEIGHBORS = hp.HParam('knn_n_neighbors', hp.IntInterval(8, 13)) # Best after hypertuning => (9, ) # Try until 20\n","HP_MODEL_KNN_WEIGHTS = hp.HParam('knn_weights', hp.Discrete([\"uniform\", \"distance\"])) # Best after hypertuning => Uniform\n","\n","knn_hparam_grid = {\n","  HP_MODEL_KNN_NEIGHBORS: range(HP_MODEL_KNN_NEIGHBORS.domain.min_value, HP_MODEL_KNN_NEIGHBORS.domain.max_value),\n","  HP_MODEL_KNN_WEIGHTS: HP_MODEL_KNN_WEIGHTS.domain.values\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3MKK2quweeT"},"source":["def hypertune_knn(session_num, train_vectors, y_train, test_vectors, y_test, hparams):\n","  for knn_hparams in list(create_tfparam_grid(knn_hparam_grid)):\n","    hparams.update(knn_hparams)\n","\n","    run_name = \"run-%d\" % session_num\n","    print('--- Starting trial: %s' % run_name)\n","    print({h.name: hparams[h] for h in hparams})\n","\n","    # Log to tensorboard\n","    with tf.summary.create_file_writer(run_dir + run_name).as_default():\n","      # Record the values used in this trial\n","      hp.hparams(hparams)  \n","      # Get metrics\n","      accuracy, f1, auc, time = get_knn_metrics(train_vectors, \n","                                  y_train, \n","                                  test_vectors,\n","                                  y_test, \n","                                  knn_hparams[HP_MODEL_KNN_NEIGHBORS], \n","                                  knn_hparams[HP_MODEL_KNN_WEIGHTS]) \n","      tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n","      tf.summary.scalar(METRIC_F1_SCORE, f1, step=1)\n","      tf.summary.scalar(METRIC_AUC, auc, step = 1)\n","      tf.summary.scalar(METRIC_TIME, time, step = 1)\n","    session_num += 1\n","\n","  return session_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NnVyu3w7qcEc"},"source":["#### Decision Tree"]},{"cell_type":"code","metadata":{"id":"JYWenuv7qbH2"},"source":["# For DT\n","HP_MODEL_DT_MAX_DEPTH = hp.HParam('dt_max_depth', hp.Discrete([10, 20]))\n","HP_MODEL_DT_CRITERION = hp.HParam('dt_criterion', hp.Discrete([\"gini\", \"entropy\"]))\n","HP_MODEL_DT_MAX_FEATURES = hp.HParam('dt_max_features', hp.Discrete([\"sqrt\", \"log2\"])) # Also here the default is None\n","\n","dt_hparam_grid = {\n","    HP_MODEL_DT_MAX_DEPTH: HP_MODEL_DT_MAX_DEPTH.domain.values,\n","    HP_MODEL_DT_CRITERION: HP_MODEL_DT_CRITERION.domain.values,\n","    HP_MODEL_DT_MAX_FEATURES: HP_MODEL_DT_MAX_FEATURES.domain.values\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFgfDF74q64q"},"source":["def hypertune_dt(session_num, train_vectors, y_train, test_vectors, y_test, hparams):\n","  for dt_hparams in list(create_tfparam_grid(dt_hparam_grid)):\n","    hparams.update(dt_hparams)\n","\n","    run_name = \"run-%d\" % session_num\n","    print('--- Starting trial: %s' % run_name)\n","    print({h.name: hparams[h] for h in hparams})\n","\n","    # Log to tensorboard\n","    with tf.summary.create_file_writer(run_dir + run_name).as_default():\n","      # Record the values used in this trial\n","      hp.hparams(hparams)  \n","      # Get metrics\n","      accuracy, f1, auc, time = get_dt_metrics(train_vectors, \n","                                  y_train, \n","                                  test_vectors, \n","                                  y_test, \n","                                  dt_hparams[HP_MODEL_DT_MAX_DEPTH],\n","                                  dt_hparams[HP_MODEL_DT_MAX_FEATURES],\n","                                  dt_hparams[HP_MODEL_DT_CRITERION])\n","      tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n","      tf.summary.scalar(METRIC_F1_SCORE, f1, step=1)\n","      tf.summary.scalar(METRIC_AUC, auc, step = 1)\n","      tf.summary.scalar(METRIC_TIME, time, step = 1)\n","    session_num += 1\n","\n","  return session_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hDjjJfkKjG4t"},"source":["#### Random Forest"]},{"cell_type":"code","metadata":{"id":"ubTbY3_njGa1"},"source":["# For RF\n","HP_MODEL_RF_N_TREES = hp.HParam('rf_n_trees', hp.Discrete([10, 20, 50]))\n","HP_MODEL_RF_MAX_DEPTH = hp.HParam('rf_max_depth', hp.Discrete([10, 20]))\n","HP_MODEL_RF_CRITERION = hp.HParam('rf_criterion', hp.Discrete([\"gini\"])) #, \"entropy\"]))\n","HP_MODEL_RF_MAX_FEATURES = hp.HParam('rf_max_features', hp.Discrete([\"sqrt\", \"log2\"]))\n","\n","rf_hparam_grid = {\n","    HP_MODEL_RF_N_TREES: HP_MODEL_RF_N_TREES.domain.values,\n","    HP_MODEL_RF_MAX_DEPTH: HP_MODEL_RF_MAX_DEPTH.domain.values,\n","    HP_MODEL_RF_CRITERION: HP_MODEL_RF_CRITERION.domain.values,\n","    HP_MODEL_RF_MAX_FEATURES: HP_MODEL_RF_MAX_FEATURES.domain.values\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFn2zGW4pfm4"},"source":["def hypertune_rf(session_num, train_vectors, y_train, test_vectors, y_test, hparams):\n","  for rf_hparams in list(create_tfparam_grid(rf_hparam_grid)):\n","    hparams.update(rf_hparams)\n","\n","    run_name = \"run-%d\" % session_num\n","    print('--- Starting trial: %s' % run_name)\n","    print({h.name: hparams[h] for h in hparams})\n","\n","    # Log to tensorboard\n","    with tf.summary.create_file_writer(run_dir + run_name).as_default():\n","      # Record the values used in this trial\n","      hp.hparams(hparams)  \n","      # Get metrics\n","      accuracy, f1, auc, time = get_rf_metrics(train_vectors, \n","                                  y_train, \n","                                  test_vectors, \n","                                  y_test,\n","                                  rf_hparams[HP_MODEL_RF_N_TREES], \n","                                  rf_hparams[HP_MODEL_RF_MAX_DEPTH],\n","                                  rf_hparams[HP_MODEL_RF_MAX_FEATURES],\n","                                  rf_hparams[HP_MODEL_RF_CRITERION])\n","      tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n","      tf.summary.scalar(METRIC_F1_SCORE, f1, step=1)\n","      tf.summary.scalar(METRIC_AUC, auc, step = 1)\n","      tf.summary.scalar(METRIC_TIME, time, step = 1)\n","    session_num += 1\n","\n","  return session_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otBZ22bCTVdg"},"source":["### Data Processing Hyperparameters\n","Developed custom param grid creator as sklearn one gave error when using tensorflow estimators as key"]},{"cell_type":"code","metadata":{"id":"LxkvVvYcYazp"},"source":["def create_tfparam_grid(hparams):\n","    dict_size = len(hparams)\n","    keys = list(hparams.keys())\n","    values = hparams.values()\n","    \n","    # Calculate possible combinations among subarrays in values\n","    combinations = itertools.product(*values)\n","    \n","    # For each combination, convert to dictionary\n","    param_grid = [dict(zip(keys, combination)) for combination in combinations]\n","    \n","    return param_grid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGKk1qwl5C84"},"source":["def filter_tfparam_grid(param_grid):\n","    new_param_grid = []\n","    for combination in param_grid:\n","      if not (combination[HP_TRANSFORM_EMOJIS_TEXT] and not combination[HP_EMOJI_INCLUDED] \\\n","        or combination[HP_TRANSFORM_EMOTICONS_TEXT] and not combination[HP_EMOTICON_INCLUDED]):\n","        new_param_grid.append(combination)\n","\n","    return new_param_grid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1z1t54hlYqSN","executionInfo":{"status":"ok","timestamp":1610135818910,"user_tz":0,"elapsed":335109,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"e87f6c5c-26e5-4db1-f103-2f97234466e0"},"source":["example_dict = { HP_EMOJI_INCLUDED: HP_EMOJI_INCLUDED.domain.values, HP_MODEL: HP_MODEL.domain.values }\n","print(create_tfparam_grid(example_dict))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[{HParam(name='emoji_included', domain=Discrete([False, True]), display_name=None, description=None): False, HParam(name='model', domain=Discrete(['mnb', 'svm']), display_name=None, description=None): 'mnb'}, {HParam(name='emoji_included', domain=Discrete([False, True]), display_name=None, description=None): False, HParam(name='model', domain=Discrete(['mnb', 'svm']), display_name=None, description=None): 'svm'}, {HParam(name='emoji_included', domain=Discrete([False, True]), display_name=None, description=None): True, HParam(name='model', domain=Discrete(['mnb', 'svm']), display_name=None, description=None): 'mnb'}, {HParam(name='emoji_included', domain=Discrete([False, True]), display_name=None, description=None): True, HParam(name='model', domain=Discrete(['mnb', 'svm']), display_name=None, description=None): 'svm'}]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-hWbyqd_ViHz"},"source":["### Run Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"id":"HarY0Tg6SWbT"},"source":["def run(session_num, train_vectors, y_train, test_vectors, y_test, hparams):\n","  if hparams[HP_MODEL] == \"svm\": \n","    return hypertune_svm(session_num, train_vectors, y_train, test_vectors, y_test, hparams)\n","  elif hparams[HP_MODEL] == \"mnb\":\n","    return hypertune_mnb(session_num, train_vectors, y_train, test_vectors, y_test, hparams)\n","  elif hparams[HP_MODEL] == \"knn\":\n","    return hypertune_knn(session_num, train_vectors, y_train, test_vectors, y_test, hparams)\n","  elif hparams[HP_MODEL] == \"dt\":\n","    return hypertune_dt(session_num, train_vectors, y_train, test_vectors, y_test, hparams)\n","  elif hparams[HP_MODEL] == \"rf\":\n","    return hypertune_rf(session_num, train_vectors, y_train, test_vectors, y_test, hparams)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lA_gJlTJJeYT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610136567971,"user_tz":0,"elapsed":1084160,"user":{"displayName":"Angel Luis Igareta Herraiz","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgcaxrBHyYPIEpw_R44fTWzYodO-r_3IUpikb82=s64","userId":"08194845434926821008"}},"outputId":"31c88793-32b1-47a4-a015-be980b067896"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-VWyAoDyUFDY"},"source":["#### Preselection of features"]},{"cell_type":"code","metadata":{"id":"rI4BHnTpRAmI"},"source":["%%time\n","session_num = 0\n","run_dir_original = '/content/drive/MyDrive/logs/'\n","run_dir = run_dir_original\n","\n","sub_df = df.loc[:, ['text', 'sentiment_numeric', 'sentiment_binary']]\n","\n","# Train-test split of 0.2\n","train, test = train_test_split(sub_df, test_size=0.2, random_state=1)\n","\n","X_train = train['text'].values\n","X_test = test['text'].values\n","\n","# Choosing sentiment_numeric or sentiment_binary as label for the models\n","for sentiment_type in HP_SENTIMENT_TYPE.domain.values[::-1]:\n","  y_train = train[sentiment_type].values\n","  y_test = test[sentiment_type].values\n","\n","  # Hyperparameters about data processing\n","  param_grid = filter_tfparam_grid(create_tfparam_grid(hparam_grid))\n","  for hparams in list(param_grid):\n","    run_dir = run_dir_original + hparams[HP_MODEL] + \"/\"\n","    hparams.update({ HP_SENTIMENT_TYPE: sentiment_type })\n","    \n","    # Preprocess train and test set with hyperparameters\n","    def preprocess_text_with_combination(text):\n","      return preprocess_text(text,\n","                              remove_emojis=not hparams[HP_EMOJI_INCLUDED], \n","                              remove_emoticons=not hparams[HP_EMOTICON_INCLUDED], \n","                              remove_hashtags=not hparams[HP_HASHTAG_INCLUDED], \n","                              remove_mentions=not hparams[HP_MENTION_INCLUDED],\n","                              remove_stop_words=not hparams[HP_STOPWORDS_INCLUDED],\n","                              transform_lowercase=hparams[HP_TRANSFORM_LOWERCASE],\n","                              transform_emojis_text=hparams[HP_TRANSFORM_EMOJIS_TEXT],\n","                              transform_emoticons_text=hparams[HP_TRANSFORM_EMOTICONS_TEXT])\n","      \n","    train_set = [preprocess_text_with_combination(text) for text in X_train]\n","    test_set = [preprocess_text_with_combination(text) for text in X_test]\n","\n","    train_vectors, test_vectors = get_train_test_vectors(hparams[HP_VECTORIZER], train_set, test_set)\n","    session_num = run(session_num, train_vectors, y_train, test_vectors, y_test, hparams)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSNPPMTnUIKV"},"source":["#### Model Hypertuning"]},{"cell_type":"code","metadata":{"id":"mApyobwtULGb"},"source":["HP_MODEL = hp.HParam('model', hp.Discrete(['svm', 'mnb', 'knn', 'dt', 'rf']))\n","\n","hparam_grid = {\n","  HP_MODEL: HP_MODEL.domain.values,\n","  HP_VECTORIZER: HP_VECTORIZER.domain.values,\n","  HP_EMOJI_INCLUDED: HP_EMOJI_INCLUDED.domain.values,\n","  HP_EMOTICON_INCLUDED: HP_EMOTICON_INCLUDED.domain.values,\n","  HP_TRANSFORM_EMOJIS_TEXT: HP_TRANSFORM_EMOJIS_TEXT.domain.values,\n","  HP_TRANSFORM_EMOTICONS_TEXT: HP_TRANSFORM_EMOTICONS_TEXT.domain.values,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIvZRJs8TMQg"},"source":["%%time\n","session_num = 0\n","run_dir_original = '/content/drive/MyDrive/logs/Models/'\n","run_dir = run_dir_original\n","\n","max_iter = 3 # Number of iterations to use cross validation\n","sub_df = df.loc[:, ['text', 'sentiment_binary']]\n","\n","# K-Fold with number of splits = max_iter\n","kf = KFold(n_splits = max_iter)\n","skf = StratifiedKFold(n_splits = max_iter, random_state = 7, shuffle = True) \n","\n","for train_index, val_index  in skf.split(np.zeros(sub_df.shape[0]), sub_df['sentiment_binary']):\n","  X_train = sub_df.iloc[train_index]['text'].values\n","  X_test = sub_df.iloc[val_index]['text'].values\n","  y_train = sub_df.iloc[train_index]['sentiment_binary'].values\n","  y_test = sub_df.iloc[val_index]['sentiment_binary'].values\n","\n","  # Hyperparameters about data processing\n","  param_grid = filter_tfparam_grid(create_tfparam_grid(hparam_grid))\n","  for hparams in list(param_grid):    \n","    # Preprocess train and test set with hyperparameters\n","    def preprocess_text_with_combination(text):\n","      return preprocess_text(text,\n","                              remove_emojis=not hparams[HP_EMOJI_INCLUDED], \n","                              remove_emoticons=not hparams[HP_EMOTICON_INCLUDED],\n","                              transform_emojis_text=hparams[HP_TRANSFORM_EMOJIS_TEXT],\n","                              transform_emoticons_text=hparams[HP_TRANSFORM_EMOTICONS_TEXT])\n","      \n","    train_set = [preprocess_text_with_combination(text) for text in X_train]\n","    test_set = [preprocess_text_with_combination(text) for text in X_test]\n","\n","    train_vectors, test_vectors = get_train_test_vectors(hparams[HP_VECTORIZER], train_set, test_set)\n","    session_num = run(session_num, train_vectors, y_train, test_vectors, y_test, hparams)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Iibob8cVo6m"},"source":["## Results"]},{"cell_type":"code","metadata":{"id":"B0lOqNFryriV"},"source":["%tensorboard --logdir /content/drive/MyDrive/logs/Models"],"execution_count":null,"outputs":[]}]}